---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Methodological considerations {#methodological-considerations}

## Reliability of micro-biopsy sampling

The micro biopsy technique used in the studies presented in this thesis generally produces smaller samples than other biopsy techniques [@RN2549]<!--Ekblom describes the Bergström and conchotome -->, and thus
requires several passes to produce sufficient material for multiple
downstream experiments. However, reports confirms that the micro-biopsy
technique is comparable to the traditionally used Bergström technique in
several measures of muscle characteristics at the same time as being much less invasive and
well-tolerated [@RN824; @RN2553].
Any reported differences in fiber type distributions between sampling techniques have been suggested to be related to differences in sampling depth [@RN2553; @RN2552].<!-- Hayot -->
Thus, any biopsy technique's reproducibility may relate to its ability to obtain sufficient material at similar depths between samples.

```{r immuno-data-prep, echo=FALSE, message=FALSE, warning=FALSE}

source("./R/libraries.R")
source("./R/themes.R")


# Sample set up (included samples)
legs <- read_csv2("./data/study-1/body-composition/oneThreeSetLeg.csv") %>%
        pivot_longer(names_to = "sets", values_to = "leg", cols = multiple:single) 


# Total number of fibers per sample
low_fiber_counts <- read_excel("./data/study-1/fiber-type/fibertype.xlsx") %>%
        inner_join(legs) %>%
        filter(include == "incl") %>%
        mutate(total = type1 + type2a + type2ax + type2x) %>%
        mutate(low_count = if_else(total < 200, "low", "acceptable")) %>%
        group_by(low_count) %>%
        summarise(n = n()) %>%
        mutate(percentage = round((n / sum(n)) * 100, 1)) 

 
average_n_fibers <- read_excel("./data/study-1/fiber-type/fibertype.xlsx") %>%
        inner_join(legs) %>%
        filter(include == "incl") %>%
        mutate(total = type1 + type2a + type2ax + type2x) %>%
        summarise(n_avg = mean(total, na.rm = TRUE)) %>%
        pull(n_avg)




between_legs <- read_excel("./data/study-1/fiber-type/fibertype.xlsx") %>%
    inner_join(legs) %>%
    filter(include == "incl") %>%
    mutate(total  = type1 + type2a + type2ax + type2x, 
           
           typeI = (type1 / total)*100,
           typeII = ((type2a + type2ax +type2x)/total) *100,
           
           type1  =(type1 / total)*100, 
           type2a =(type2a / total)*100, 
           type2ax=(type2ax / total)*100, 
           type2x =(type2x / total)*100 ) %>% 
    
    filter(total > 300) %>%
    
    filter(timepoint %in% c(1)) %>%
    dplyr::select(subject, timepoint, leg,total, type1:type2x, typeI:typeII) %>%
    pivot_longer(names_to = "type", values_to = "percentage", cols = c(type1:type2x, typeI:typeII)) %>%
    mutate(timepoint = paste0("t", timepoint)) %>%
    pivot_wider(names_from = leg, values_from = c(percentage, total)) %>%
    
    mutate(type = factor(type, levels = c("type1", 
                                          "type2a", 
                                          "type2ax", 
                                          "type2x", 
                                          "typeI", 
                                          "typeII"), 
                         labels = c("Type I", 
                                    "Type IIA", 
                                    "Type IIAX", 
                                    "Type IIX", 
                                    "Type I_tot", 
                                    "Type II_tot"))) %>%
     filter(complete.cases(.)) %>%
        rowwise() %>%
        mutate(mean.perc = mean(c(percentage_L, percentage_R)), 
               diff.perc = percentage_L - percentage_R, 
               diff.sq = (percentage_L - percentage_R)^2,
               diff.mean.sq = (diff.perc/mean.perc)^2,
               min.count = min(c(total_L, total_R))) %>%
        filter(mean.perc > 0) %>%
        
        group_by(type) %>%
        
        summarise(m = mean(mean.perc), 
                  S2 = sum(diff.sq)/(2*n()), 
                  S = sqrt(S2), 
                  RMS = 100 * sqrt(sum(diff.mean.sq) / (2*n())), 
                  ws = 100 * (sqrt(sum(diff.perc^2) / (2*n()) ))/m, 
                  n = n()) %>%
        mutate(cv  = 100 * (S/m)) 
     


 
between_duplicates <-  read_excel("./data/study-1/fiber-type/fibertype.xlsx") %>%
         inner_join(legs) %>%
         filter(include == "incl") %>%
         mutate(total  = type1 + type2a + type2ax + type2x, 
                type1  =(type1 / total)*100, 
                type2a =(type2a / total)*100, 
                type2ax=(type2ax / total)*100, 
                type2x =(type2x / total)*100) %>%  
     
    filter(total > 300) %>%
         filter(timepoint %in% c(2, 3)) %>%
         dplyr::select(subject, timepoint, leg,total, type1:type2x) %>%
         pivot_longer(names_to = "type", values_to = "percentage", cols = type1:type2x) %>%
         mutate(timepoint = paste0("t", timepoint)) %>%
         pivot_wider(names_from = timepoint, values_from = c(percentage, total)) %>%
         
 
         mutate(type = factor(type, levels = c("type1", 
                                               "type2a", 
                                               "type2ax", 
                                               "type2x"), 
                              labels = c("Type I", 
                                         "Type IIA", 
                                         "Type IIAX", 
                                         "Type IIX"))) %>%
     filter(complete.cases(.)) %>%
         
         rowwise() %>%
         mutate(mean.perc = mean(c(percentage_t2, percentage_t3)), 
                diff.perc = percentage_t2 - percentage_t3, 
                diff.sq = (percentage_t2 - percentage_t3)^2,
                diff.mean.sq = (diff.perc/mean.perc)^2,
                min.count = min(c(total_t2, total_t3))) %>%
         filter(mean.perc > 0) %>%  
     group_by(type) %>%
     
     summarise(m = mean(mean.perc), 
               S2 = sum(diff.sq)/(2*n()), 
               S = sqrt(S2), 
               RMS = 100 * sqrt(sum(diff.mean.sq) / (2*n())), 
               ws = 100 * (sqrt(sum(diff.perc^2) / (2*n()) ))/m, 
               n = n()) %>%
     mutate(cv  = 100 * (S/m)) 






```

In Study I, one or several pieces of muscle (total weight
$\sim$\SI{15}{mg}) were chosen for analyses of muscle fiber type composition per muscle biopsy sampling event. The average total number of counted fibers per sample was `r round(average_n_fibers, 0)`.
Only a small percentage (`r low_fiber_counts[2,3]`\%) of samples did not contain enough fibers for representative analysis of fiber type composition (> 200 fibers),
[@RN874]<!--Blomstrand suggests duplicate samples and at least 200 fibres for determination of fiber distributions -->
and as many as in some specimen 1400 fibers were counted (\@ref(fig:fiber-methods-fig)a).
The smaller amount of tissue obtained using the micro-biopsy technique did not limit fiber type composition analyses.

In order to assess the reproducibility of the technique, samples obtained during the same day (separated by $\sim$ 2 hours) were analyzed as duplicate samples. 
Similar to Blomstrand *et al.* [@RN874], variations from duplicate measures were calculated as:
$$S = \sqrt{\frac{\sum{d^2}}{2n}}$$
Where $d$ is the difference between paired observations and $n$ is the number of pairs.
The variation between duplicate samples were `r round(between_duplicates[1,4],2)` and `r round(between_duplicates[2,4],2)`%-points for Type I and IIA fibers, respectively (\@ref(fig:fiber-methods-fig)b).
This resembled variations between leg (Type I, `r round(between_legs[1,4],2)`; Type IIA, `r round(between_legs[2,4],2)`; Figure \@ref(fig:fiber-methods-fig)c).
These estimates of variation are slightly lower than those reported by Blomstrand *et al.* [@RN874]<!--Blomstrand suggests duplicate samples and at least 200 fibres for determination of fiber distributions -->, using the Bergström needle, and suggest that the micro-biopsy technique provides reproducible results of fiber type composition in young adults.

```{r fiber-methods-fig, fig.cap="Number of fibers in immunohistochemistry analyses (a), correlation between duplicate samples from the same leg (separated by 2 hours) (b) and correlations between samples from left and right leg pre-intervention.", fig.scap="Characteristics of biopsy samples used in immunohistochemistry analyses.", warning=FALSE, echo = FALSE, warning=FALSE, fig.width = 3.5, fig.height = 5, fig.align='center'}



 ############## Figures myhc fiber typing  ######################      
total_histogram <- read_excel("./data/study-1/fiber-type/fibertype.xlsx") %>%
        inner_join(legs) %>%
        filter(include == "incl") %>%
        mutate(total = type1 + type2a + type2ax + type2x) %>%        
        
        ggplot(aes(total)) + geom_histogram(binwidth = 30, 
                                            fill = group.study.color[5], alpha = 0.8) + 
        dissertation_theme() + 
        scale_x_continuous(limits = c(0, 1600),
                           expand = c(0,0),
                           breaks = c(0, 200, 400, 600, 800, 1000, 1200, 1400, 1600), 
                           labels = c(0, "", 400, "", 800, "", 1200, "", 1600)) + 
        scale_y_continuous(limits = c(0, 20), 
                           breaks = c(0, 5, 10, 15, 20), 
                           expand = c(0,0)) +
        
        labs(x = "Total number of fibers per sample", 
             y = "Counts") + 
        theme(axis.text.x = element_text(hjust = c(0, rep(0.5, 7), 1)))
        
### Comparison between "duplicate samples" at week 2
corr_plot_fiber_types <- read_excel("./data/study-1/fiber-type/fibertype.xlsx") %>%
        inner_join(legs) %>%
        filter(include == "incl") %>%
        mutate(total  = type1 + type2a + type2ax + type2x, 
               type1  =(type1 / total)*100, 
               type2a =(type2a / total)*100, 
               type2ax=(type2ax / total)*100, 
               type2x =(type2x / total)*100) %>%  
        filter(timepoint %in% c(2, 3)) %>%

        dplyr::select(subject, timepoint, leg,total, type1:type2a) %>%
        pivot_longer(names_to = "type", values_to = "percentage", cols = type1:type2a) %>%
        mutate(timepoint = paste0("t", timepoint)) %>%
        pivot_wider(names_from = timepoint, values_from = c(percentage, total)) %>%
        ungroup() %>%
        mutate(type = factor(type, levels = c("type1", 
                                              "type2a"), 
                             labels = c("Type I", 
                                        "Type IIA"))) %>%
     
        ggplot(aes(percentage_t2, percentage_t3)) + 
        geom_point(shape = 21,
                   size = 1.8,
                   alpha = 0.7,
                   fill = group.study.color[4]) + 
        
        facet_wrap(~type, ncol = 2, strip.position = "top")  +
        
        scale_x_continuous(limits = c(0, 100), 
                           expand = c(0,0), 
                           breaks = c(25, 50 ,75)) + 
        scale_y_continuous(limits = c(0, 100), 
                           expand = c(0,0), 
                           breaks = c(25, 50 ,75)) + 
        
        labs(x = "Duplicate 1 (% of total fibers)", 
             y = "Duplicate 2\n(% of total fibers)") +
        
        geom_abline(intercept = 0, slope = 1, lty = 2, color = "gray50") + 
        dissertation_theme() + 
        theme(strip.background = element_blank(),
              strip.text = element_text(size = 8, hjust = 0),
              panel.border = element_rect(color = "black", fill = NA))
        
### Left right at baseline 

baseline_between_legs <- read_excel("./data/study-1/fiber-type/fibertype.xlsx") %>%
        inner_join(legs) %>%
        filter(include == "incl") %>%
        mutate(total  = type1 + type2a + type2ax + type2x, 
               type1  =(type1 / total)*100, 
               type2a =(type2a / total)*100, 
               type2ax=(type2ax / total)*100, 
               type2x =(type2x / total)*100) %>%  
        filter(timepoint %in% c(1)) %>%

        dplyr::select(subject, timepoint, leg,total, type1:type2a) %>%
        pivot_longer(names_to = "type", values_to = "percentage", cols = type1:type2a) %>%
        pivot_wider(names_from = leg, values_from = c(percentage, total)) %>%
        ungroup() %>%
        mutate(type = factor(type, levels = c("type1", 
                                              "type2a"), 
                             labels = c("Type I", 
                                        "Type IIA"))) %>%
     
        ggplot(aes(percentage_L, percentage_R)) + 
        geom_point(shape = 21,
                   size = 1.8,
                   alpha = 0.7,
                   fill = group.study.color[2]) + 
        
        facet_wrap(~type, ncol = 2, strip.position = "top")  +
        
        scale_x_continuous(limits = c(0, 100), 
                           expand = c(0,0), 
                           breaks = c(25, 50 ,75)) + 
        scale_y_continuous(limits = c(0, 100), 
                           expand = c(0,0), 
                           breaks = c(25, 50 ,75)) + 
        
        labs(x = "Left leg (% of total fibers)", 
             y = "Right leg\n(% of total fibers)") +
        
        geom_abline(intercept = 0, slope = 1, lty = 2, color = "gray50") + 
        dissertation_theme() + 
        theme(strip.background = element_blank(),
              strip.text = element_text(size = 8, hjust = 0),
              panel.border = element_rect(color = "black", fill = NA))


fiber_type_rel_plot <- plot_grid( plot_grid(NULL, total_histogram, NULL, ncol = 3, rel_widths = c(0.2, 1, 0.2)),
          corr_plot_fiber_types, 
          baseline_between_legs,
          ncol = 1, rel_heights = c(1.5, 2, 2)) + 
        
        draw_plot_label(label=c("a", "b", "c"),
                        x = c(0.02, 0.02, 0.02), 
                        y = c(0.98, 0.75, 0.35),
                        hjust=.5, vjust=.5, 
                        size = label.size)

fiber_type_rel_plot
```

## Model-based normalization of qPCR data

Quantitative reverse-transcription real-time polymerase chain-reaction (qPCR) is the method of choice for conducting targeted gene-expression studies.
After reverse-transcription of RNA, complementary DNA (cDNA) is amplified in the presence of target-specific primers and a reporter probe or dye that produces fluorescence as the PCR product accumulates over repeated PCR cycles. When sufficient amounts of product is formed, the fluorescent signal rises above the background fluorescence, and the fractional PCR cycle used for quantification (Cq) can be estimated. The Cq-values is directly proportional to the starting amount of the targeted transcript and thus constitute the primary outcome of qPCR experiments
[@RN2099].
qPCR experiments should ideally be designed to account for methodological pitfalls, including amounts of RNA utilized for reverse-transcription, the efficiency of cDNA synthesis, and the amount of cDNA used in PCR amplification.
As traditional statistical treatment of qPCR data accounts for these challenges through the use of reference genes [@RN2100; @RN2101], the stable expression of such a gene is an important assumption in the experiment.
Using an internal control-factor comprised of the geometric average of multiple reference-genes has been shown to increase the validity of this assumption [@RN991].
However, the stability of potential reference genes must be determined in each experiment [@RN1983; @RN991], highlighting that qPCR is inevitably challenging, with specific challenges being posed by the biological system under study.
Despite this, recent investigations of human skeletal muscle in response to exercise, reported using a single-reference gene, validated or non-validated for normalization, despite the recommended use of a validated set of reference genes [@RN1985; @RN2083; @RN2084; @RN2086; @RN2087].
<!-- Recent studies using single reference genes -->
<!-- Only GAPDH in  [@RN2083] -->
<!-- only GAPDH in  [@RN2084]  validated in  [@RN2085] -->
<!-- only GAPDH in (not validated)  [@RN2086] -->
<!-- only GAPDH (stable across time) [@RN2087] -->


Several methods have been proposed to select a suitable set of references genes, out of which geNorm [@RN991] and Normfinder [@RN1772] are the two most common approaches. 
The geNorm algorithm assesses reference-gene stability through an iterative calculation of pairwise expression-ratios where the least stable gene, i.e., the gene that varies the most relative to other genes across all samples, is removed in each iteration, resulting in a set of at least two genes that exhibit the least pairwise variation [@RN991].
As no information about the study design can be incorporated into geNorm, a potential pitfall is a variable expression of potential reference genes across experimental groups or conditions [@RN1772].
The Normfinder algorithm utilizes a linear model to determine variation in reference-gene expression as deviations from an assumed stable expression.
During Normfinder analyses a grouping factor can be included, resulting in a stability measure that combines variation within and between groups for each gene.
This inclusion does not, however, relax the assumption of no *systematic* variation across experimental conditions [@RN1772].

As many experiments performed in exercise physiology, especially related to muscle, are conducted as repeated measurement studies, where the same participants are investigated over time, the question regarding reference-gene stability may be slightly rephrased. Instead of searching for the most stable gene across the whole data set, we may want to assess a set of genes' stability within each participant.
Dai and co-workers [@RN1771] suggested the utilization of a mixed-effects model-based selection algorithm.
First, the algorithm determines whether variation in expression of a potential set of reference-genes depends on experimental conditions.
Then, it determines the expression stability of gene-sets through the intra-class correlation within data clusters, e.g., study participants, where gene expression is to be monitored over repeated measures.
Such a mixed-effect model approach accounts for and takes advantage of correlated measures.
The proposed algorithm can also capture complex experimental designs and confirm that potential genes are not systematically affected by conditions studied. 

In Study I, to assess the stability of potential reference genes, 11 genes were evaluated (Figure \@ref(fig:ref-gene-comp)a). All three selection algorithms suggested different reference genes (Figure \@ref(fig:ref-gene-comp)b). The mixed-model approach resulted in a set of genes that showed the least systematic variation across study conditions. Figure \@ref(fig:ref-gene-comp)c shows coefficients with 95\% CI from mixed models with each normalization factor as the dependent variable. The naive use of the commonly utilized GAPDH as a single reference gene would result in the largest deviation from the assumption of a stable reference over time, as it was expressed to a lesser degree before exercise at Week 2 compared to baseline. However, when comparing between conditions, normalization factors suggested by geNorm and Normfinder both showed differentiated levels between conditions before exercise at Week 2 indicating that they were unsuited for use as reference (Figure \@ref(fig:ref-gene-comp)c). These results underline that bias can be introduced in qPCR experiments from the choice of normalization factor. The use of a selection algorithm that tests explicitly for stability within participants may provide a more robust normalization factor.


```{r ref-gene-comp, fig.cap="Variation in normalization factors determined by mixed-model selection, Normfinder and geNorm as well as comprised of a single-gene (GAPDH) for comparison (a). Estimates from mixed-effects regression-models for each normalization factor (b). Error bars represent 95\\% confidence intervals.", fig.scap="Reference gene selection in Study I", warning=FALSE, echo = FALSE, warning=FALSE, fig.width = 4.72, fig.height = 6, fig.align='center'}

selection_process <- readRDS("./data/derivedData/study1-reference-gene-stability/selection_process.RDS")


gene_descriptive <- readRDS("./data/derivedData/study1-reference-gene-stability/gene_descriptive.RDS")



ref_set_comp <- readRDS("./data/derivedData/study1-reference-gene-stability/ref_set_comp.RDS")

plot_grid(gene_descriptive, ref_set_comp, ncol = 1, rel_heights = c(0.4, 1)) +
  draw_plot_label(c("a", "b", "c"), 
                  x = c(0.0, 0.0, 0.5), 
                  y = c(0.99, 0.72, 0.72))






```

The interpretation of qPCR data indirectly relates to the choice of biological input. When, e.g., total RNA is extracted and normalized per sample before cDNA synthesis, stable reference genes are assumed to be stable per total RNA. This logic also applies when a fixed amount of tissue or mRNA is used as an input quantity
[@RN1984].<!-- Bustin -->
Internal reference genes will if they are validated, thus reflect the input quantity. The interpretation in such a case is that target gene abundance changes per amount of biological input, e.g., total RNA.
As such, the validation of a set of reference genes may be redundant as normalization is performed when an equal amount of RNA is subjected to cDNA synthesis.
Additionally, reference genes are also measured with technical error and biological variation that is inevitably introduced to the analysis.
Modifications to the design of the qPCR experiment were suggested by Steibel and co-workers [@RN1154], who utilized a linear mixed-effects model framework where the model was specified with a random-effects structure designed to capture variations due to e.g. sample preparation as well as the overall study design (though fixed-effects).
Technical variation was modeled as random effects of two or more technical replicates.
In this approach, all target genes are modeled together as a set, and the effect of experimental conditions are evaluated from the model for each gene-of-interest.
When utilizing a linear mixed-effects model, as suggested by Steibel *et al.* [@RN1154], assumptions are made that errors are normally distributed, and variance is equal over the range of the data (homoscedasticity). 
This assumption can be problematic when dealing with low-abundance genes, as the variation increases when few target molecules are amplified [@RN1964].
Such model assumptions can be checked and delt with using appropriate statistical methods allowing for heterogeneous variance [@RN1986].
Further extending the concept described by Steibel *et al.*  [@RN1154], Matz and co-workers [@RN1964] proposed modeling transcript abundance with a Poisson-lognormal generalized linear mixed-model. This model enables one to account for the specific characteristics of qPCR data, including no amplification (zero target molecules) and heteroscedasticity, i.e., larger variance due to low-abundance starting material.
Matz *et al.*  also suggested that the random-effects structure would provide sufficient within-model normalization [@RN1964].

As suggested by Steibel *et al.* and Matz *et al.*, the use of model-based normalization represents a promising alternative in situations where the appropriate validation of reference genes is not feasible.
However, the implementation of such analysis comes with a new set of assumptions.
First, if a study condition systematically leads to lower amounts or quality of cDNA due to e.g. RNA degradation, a random effect structure will not account for this but instead result in estimates of lower expression for all genes in such samples. In such case, the use of internal reference genes will yield different results as the internal control will also be affected by RNA degradation
[@RN1964].
This fact leads to the assumption in a model-based normalization strategy that variations in cDNA quantity or quality between samples are randomly distributed.
Additional assumptions revolve around the choice of statistical model.
In the basic parameterization of a linear mixed-effects model, homoscedasticity and normal distribution of the residual errors are assumed. 
As differently expressed genes can be assumed to have different overall variances, this would violate the assumption of homoscedasticity. To account for possible heterogeneity among different genes, a variance function
[@RN1986]
can be introduced to the model to allow for gene-specific residual variance. This approach can be extended with variance functions that also account for higher variation due to the stochastic processes when amplifying small amounts of starting material.
Such gene-specific residual variance can be modeled taking the observed Ct-value into account.
Adding variance-functions to the model could be regarded as an analog to the use of a Poisson log-normal model to account for qPCR specific technical variability, as suggested by Matz *et al.*[@RN1964].

Implementations of model-based normalization can be accomplished both in a frequentist and Bayesian framework.
The `nlme` package provides functions to fit linear mixed-effects models (LMM) with extended variance structures that can account for gene-specific residual variance [@RN1986].
Matz *et al.* [@RN1964] used the `MCMCglmm` [@RN1992] package to fit Bayesian generalized linear mixed models (GLMM) using a Poisson log-normal errors.  

The introduction of more elaborate random effect-structures and variance structures adds parameters to each model that has to be estimated. This complexity may lead to convergence issues in linear mixed-effects models
[@bates2018parsimonious].
Analogous to this, in a Bayesian framework, Markov chain Monte Carlo (MCMC) sampling may struggle to converge when models are too complex. 
The robustness of modeling algorithms may thus represent a limitation in terms of reproducibility of model-based normalization.
To assess the robustness of model-based normalization strategies, all possible gene combinations of sizes 2 and 11 (both n = 78) from a set of 13 genes investigated in Study I were used to fit models using LMM and GLMM approaches.
The same random effect structure was used in both frameworks.
The fraction of models that did not converge (LMM) or showed estimation issues (LMM and GLMM) was determined for both modeling strategies. In the LMM approach, non-estimable effects were defined when the algorithm could not approximate the covariance matrix. In the GLMM approach, a Gelman-Rubin convergence criterion > 1.1 from two chains for any estimate was used as a threshold to define estimation issues.
The analysis showed that the number of models with convergence issues increased as the number of targets increased from two to eleven \@ref(fig:qpcr-model-robustness). To circumvent such convergence issues, simplification of random effect-structures or reducing the number of targets may help LMM and GLMM. Adding MCMC iterations in the Bayesian framework could improve its performance.


```{r qpcr-model-robustness, fig.cap = "Increasing the number of targets in gene-set modeling increases the rates of non-convergence and estimation issues in LMM as wells as estimation issues in GLMM. Estimation issues in LMM was defined when variance components were not estimable, in the GLMM approach the Gelman-Rubin criterion was used to define estimation issues.", fig.scap="Robustness of model-based qPCR normalization", fig.width = 4, fig.height=2.5, fig.align='center'}

robust2genes <- read_rds("./data/study-1/qpcr/reference-gene-selections/robust_2genes.RDS")
robust11genes <- read_rds("./data/study-1/qpcr/reference-gene-selections/robust_11genes.RDS")

robust2genes <- robust2genes %>%
  mutate(mcmc.convergance = paste0(mcmc.vcv.convergance, mcmc.sol.convergance) ) %>%
  dplyr::select(lme.convergance, mcmc.convergance) %>%
  gather(model, conv, lme.convergance:mcmc.convergance) %>%
  count(model, conv) %>%
  mutate(p = (n / 78)*100,
         ngenes = 2) 

robust11genes <- robust11genes %>%
  mutate(mcmc.convergance = paste0(mcmc.vcv.convergance, mcmc.sol.convergance) ) %>%
  dplyr::select(lme.convergance, mcmc.convergance) %>%
  gather(model, conv, lme.convergance:mcmc.convergance) %>%
  count(model, conv) %>%
  mutate(p = (n / 78)*100,
         ngenes = 11) 

  
robust <- rbind(robust2genes, robust11genes)

robust_fig <- robust %>%
  mutate(Performance = if_else(conv == "VCVgoodSolgood" |conv == "Estimate", "Convergence",
                               if_else(conv == "NoConvergance", "No convergence", "Estimation\nissues"))) %>%
  mutate(Performance = factor(Performance, levels = c("Convergence", "Estimation\nissues", "No convergence")),
         model = factor(if_else(model == "lme.convergance", "LMM", "GLMM"), levels = c("LMM", "GLMM")),
         ngenes = factor(if_else(ngenes == 2, "Two targets", "Eleven targets"), 
                         levels = c("Two targets", "Eleven targets"))) %>%
  group_by(model, ngenes, Performance) %>%
  summarise(p = sum(p)) %>%
  ggplot(aes(model, p, fill = Performance)) + geom_bar(stat="identity") +
  scale_fill_manual(values = c("gray90", "gray60", "gray20")) +
  ylab("Percentage of\n models") + xlab("Modeling strategy")+
  
  scale_y_continuous(limits = c(0, 101),
                     expand = c(0, 0),
                     breaks = c(0, 25, 50, 75, 100)) +
  
  facet_grid(. ~ ngenes) +
  guides(fill = guide_legend(nrow=3,byrow=TRUE)) +
  dissertation_theme() +
  theme(strip.text = element_text(size = 7), 
        strip.background = element_blank(), 
        legend.text = element_text(size = 7), 
        legend.title = element_text(size = 7))
  
robust_fig

```

The choice of a model-based normalization approach for qPCR analysis could also be based on statistical efficiency. As measurements of reference genes are associated with technical error and biological variation, relying on these measurements affects subsequent statistical tests' power. To explore the effect of model-based vs. reference-gene based normalization on statistical power, a small simulation study was performed.
First, a simulated data-set of Ct-values for n = 1000 participants and four genes was created, designed to mimic the observed data-set in Study I. Random effects, common to all genes were added trough random sampling from distributions with zero mean and given standard deviations for participants ($\sigma = 0.3$), participants legs ($\sigma = 0.001$) and biological samples within-participant ($\sigma = 0.27$). Gene-specific random effects for each participant ($\sigma = 0.25$), leg ($\sigma = 0.0001$) and biological sample ($\sigma = 0.1$) were added. Gene-specific residual error was then added to each replicate where the sampling distributions were set to $\sigma=0.2$ and $\sigma=0.6$ in the genes-of-interest and $\sigma=0.01$ and $\sigma=1$ in the "stable" and "unstable" reference gene respectively. To further mimic the behavior of qPCR data, an additional random error was added where the sampling distribution had a standard deviation of $\sigma = 0.2 \times Ct-min(Ct)$, which gave heteroscedasticity within each gene related to the simulated Ct. Together these parameters simulate the randomness of a qPCR data set with gene-specific heteroscedasticity over the range of simulated Ct-values with increased error in genes that were simulated as less abundantly expressed (higher Ct-values).
All genes where given the same average Ct-value of 20 and two genes where considered genes of interest where fixed-effects of time and interaction effects between time and condition were added representing fold-changes of ~1.15-1.52 (differences in Ct-values of 0.2-0.6). Participants (n = 30) were randomly sampled from the simulated population and gene expression values were assessed either in model-based normalization approaches or a gene-by-gene manner where a single target was normalized to a reference gene and modeled using the same fixed effect structure as in the model-based approach. 

The model-based normalization approaches were comparable and consistently better performing than the gene-by-gene strategy wherein target genes were normalized by reference-genes (Figure \@ref(fig:qpcr-simulations)). Adding random variation to the target gene moved the model-based approach closer to the reference-gene normalization strategy (comparing $\sigma=0.2$ with $\sigma=0.6$). On the other hand, adding random variation to the reference-gene naturally decreased statistical power compared to the stable versus the unstable reference gene.

In summary, the choice of analytic methods is potentially a source of significant variation in analyses of qPCR data and subsequent inference. 
These notes underline that establishing and critically evaluating data-analytic pipelines adapted to specific experimental designs can explicitly test assumptions about both biological, technical, and statistical aspects of qPCR analysis.
Although using a model-based normalization approach could prove problematic in terms of model fitting, it is likely more accurate and powerful.

```{r qpcr-simulations, fig.cap = "Statistical power increases with effect-size. Effect sizes were assessed as main effects of time (left panels) or as interaction between time and group (right panels). Gene-specific variation in each target gene was set to $\\sigma=0.2$ (upper panels) and $\\sigma=0.6$ (lower panels).", fig.scap="qPCR power-simulation", fig.width = 3, fig.height=3, fig.align='center'}



simresults <- bind_rows(read_rds("./data/study-1/qpcr/reference-gene-selections/simresults30.RDS"))
  


simulation_results <- simresults %>%
filter(coef != "(Intercept)") %>%
  mutate(pwr = 0,
         pwr = if_else(coef == "timepointt2" & lwr > 0, 1, pwr),
         pwr = if_else(coef == "timepointt3" & lwr > 0, 1, pwr),
         pwr = if_else(coef == "timepointt4" & lwr > 0, 1, pwr),
         pwr = if_else(coef == "timepointt2:conditionc2" & lwr > 0 | upr < 0, 1, pwr),
         pwr = if_else(coef == "timepointt3:conditionc2" & upr < 0, 1, pwr),
         pwr = if_else(coef == "timepointt4:conditionc2" & upr < 0, 1, pwr),
         pwr = if_else(coef == "conditionc2" & lwr > 0 | upr < 0, 1, pwr),
         beta = if_else(coef == "timepointt2" & gene %in% c("g1", "g2"), 2^0.3,
                        if_else(coef == "timepointt3" & gene %in% c("g1", "g2"), 2^0.5,
                                if_else(coef == "timepointt4" & gene %in% c("g1", "g2"), 2^0.7,
                                        if_else(coef == "timepointt3:conditionc2" & gene %in% c("g1", "g2"), 2^0.3,
                                                if_else(coef == "timepointt4:conditionc2" & gene %in% c("g1", "g2"), 2^0.5, 0))))),
         effect = if_else(coef %in% c("timepointt2", "timepointt3", "timepointt4"), "main", "interaction")) %>%
  group_by(method, gene, coef, beta, effect) %>%
  
  summarise(pwr = sum(pwr)/n()) %>%
  filter(gene %in% c("g1", "g2") & beta > 0.2) %>%
  ungroup() %>%
  mutate(effect = factor(effect, levels = c("main", "interaction"), labels = c("Main", "Interaction")),
         Strategy = factor(method, levels = c("nlme", "mcmc", "g3", "g4"), 
                           labels = c("LMM", "GLMM", "Stable\nreference", "Unstable\nreference")),
         gene = factor(gene, labels = c(expression(paste(sigma," = 0.2")),
                                        expression(paste(sigma," = 0.6"))))) %>%
  
  ggplot(aes(beta, pwr, color = Strategy,shape = Strategy, group = paste0(effect, method))) + 
  
  geom_point(size = 2.5, position = position_dodge(width=0.01)) + 
  
  geom_line(position = position_dodge(width=0.01)) +
  
  facet_grid(gene~effect, labeller = label_parsed) + 
  
  ylab(expression(paste("Power (1-",beta," error probability)"))) + xlab("Effect size (Fold-change)") +
  
  guides(color=guide_legend(keywidth=0.2, keyheight=0.4, default.unit="inch")) +
  
  scale_color_manual(values = group.study.color[c(1, 2, 3, 5)]) +
  
  dissertation_theme() + 
  theme(strip.background = element_blank(), 
        strip.text.x = element_text(size = 7), 
        strip.text.y = element_text(size = 7, angle = 0), 
        legend.position = "bottom", 
        legend.title = element_blank())
  
simulation_results


```


## Increased relevance of RNA-sequencing data through data-driven selection of analysis tools 

Analysis of high-dimensional data such as those retrieved from RNA sequencing experiments requires the use of a multitude of bioinformatic and statistical tools. Such tools are continuously being developed and optimized for different tasks. This continuing development requires that a combination of tools is validated for specific study conditions [@RN2426]<!-- Conesa 2016 pipelines need to be adopted to study specific conditions-->
To improve the interpretability of the RNA sequencing data obtained from Study I, we sought to compare mapping tools for their validity. As a first step, we utilized myosin heavy-chain compositions determined by immunohistochemistry to relate mRNA counts to protein levels. Myosin heavy-chains are well suited for this purpose as their mRNA and protein levels are known to correlate in steady state conditions
[@RN2444; <!-- Serrano Correlation between mRNA and prot abundance MyHC -->
@RN1489; <!-- Ellefsen Reliable determination of myosin heavy chain -->
@RN2445].<!-- Marx 2002 correlations were observed between the isoform expression of different MyHCs at the protein and mRNA-->
To avoid concerns regarding normalization assumptions, both mRNA and protein abundances were expressed relative to the sum of counts for the whole gene- and protein-family 
[@RN825; <!--Ellefsen Gene family profiling  -->
@RN1489].
This analysis indicated that transcript-based mapping tools
(RSEM [@RN2387]<!-- Li Rsem-->,
kallisto [@RN2389]<!-- Bray kallisto --> and Salmon [@RN2390])
resulted in stronger correlations between mRNA and protein profiles than genome-based mapping tools
(STAR [@RN2386]<!-- Dobin STAR -->
and  HISAT2 [@RN2385]; Figure \@ref(fig:rna-seq-myhc-validation)).

```{r rna-seq-myhc-validation, fig.cap="Validation of mapping tools using mRNA to protein correlations of myosin heavy-chain genes", fig.scap="Correlations between gene-family normalized protein and gene data from different mRNA quantification methods.", warning=FALSE, echo = FALSE, warning=FALSE, fig.width = 4.72, fig.height=7, fig.align='center'}

myhc_rnaseq_fig <- readRDS("./data/derivedData/study1-myhc-rnaseq-validation/myhc_rnaseq_fig.RDS")

myhc_rnaseq_fig

```

A second analysis took advantage of the within-participant design of the study, whereby the assumption was that paired observations between legs, measured before the intervention would result in similar expression profiles if read mappings were more accurate. The pairwise Log\textsubscript{2}-differences were determined of a set of genes with known robust expression across tissues [@RN1759].<!-- Eisenberg housekeeping -->
Across the whole range of transcript abundances (Average Log\textsubscript{2}-count), RSEM displayed smaller differences between pairwise observations, indicating less technical variation. RSEM differed marginally from other transcript-based tools but to a larger degree to genome-based tools (Figure \@ref(fig:rna-seq-pairwise-validation)).

Mapping tools were used with default settings, which leaves room for further optimization of each tool. However, selecting tools with default settings should provide a heuristic solution to the problem of optimizing an analytic pipeline. Taken together, these observations highlight that RNA sequencing experiments can be improved in terms of biological validity by systematic evaluation of analytic tools.  



```{r rna-seq-pairwise-validation, fig.cap="Average log-differences across average abundance levels for different mapping tools from paired samples retrieved prior to the intervention in Study I. Less difference indicate lower technical variation, assuming similarities between legs.", fig.scap="Within-participant variation between RNA sequencing mapping tools.", warning=FALSE, echo = FALSE, warning=FALSE, fig.width = 2.5, fig.height=2.5, fig.align='center'}

rna_seq_validation_pairwise <- readRDS("./data/derivedData/study1-pairwise-variation-rnaseq/pairwise_fig.RDS")


rna_seq_validation_pairwise


```












